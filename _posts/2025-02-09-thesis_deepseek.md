---
title: "[논문] Deepseek 논문 정리"
description: 
author:
date: 2025-02-09 23:00:00 +0900
categories: [논문, deepseek]
tags: [논문]
pin: false
math: true
mermaid: true
image:
  # path: /assets/img/20250118_post/KT_모집요강.JPG
  # lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
  # alt: ktaivel_image
---

## **0. DeepSeek 그것은 무엇인가**
<hr style="height: 0.5px; background-color: rgba(0, 0, 0, .1); border: none;" />
중국에서 24년도 중순부터 개발된 인공지능 모델로 25년 1월경에 발표가 이루어지며 사회적으로 큰 영향을 끼치고 있는 모델입니다.  
우선 가장 눈에띄는 차이점은 아래와 같습니다.
1. 뛰어난 성능 : 현재까지는 대부분의 항목에서 GPT-4를 능가하는 성능을 보여줍니다.
2. 빠른 응답속도 : 추론기반으로 빠른 응답속도를 보여줍니다.
3. 오픈소스 : 소스코드가 공개되어있어 개발자들이 자유롭게 모델링하며 개선할 수 있습니다.
4. 효율적인 모델구조 : MoE 아키텍처를 사용하여 대규모 파라미터를 효율적으로 관리합니다.
이중 사회적으로 가장 이슈가되고있는 모델 구조에대해서 이번에는 알아보고자합니다.

## **1. MoE 모델이 뭔데?**
<hr style="height: 0.5px; background-color: rgba(0, 0, 0, .1); border: none;" />
MoE 모델이란 단순히 말하면 아래와 같습니다.


## **2. 지원 자격**
<hr style="height: 0.5px; background-color: rgba(0, 0, 0, .1); border: none;" />

1. 전공 무관 // 하지만 기초적인 코딩역량 필요
2. 졸업자 혹은 졸업 예정자 (34세 이하)
3. 국민 내일 배움카드 발급 가능장(중요! kdt 수강 이력이 없어야 합니다.)
3. 미취업자(교육 시작일 기준)

이외에 자세한 사항은 아래와 같습니다.

![Desktop View](/assets/img/20240912_post/지원자격.PNG){: width="800" height="400"}
_KT Aivle School 지원자격_


## **3. 마무리리**
<hr style="height: 0.5px; background-color: rgba(0, 0, 0, .1); border: none;" />

취업걱정되시는분! 혹은 요즘 트렌드인 AI시장에 발을 들이고 싶으신 분들은 한번 참여해보셔도 좋은 프로그램입니다!

## **3. 출처**
<hr style="height: 0.5px; background-color: rgba(0, 0, 0, .1); border: none;" />

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)  